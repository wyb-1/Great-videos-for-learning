# 第三代神经网络：脉冲神经网络

## 1、最近相关机器学习顶会视频

- [20200601:通过混合转换训练深度脉冲神经网络（SNN）](https://www.youtube.com/watch?v=LwQ-0liDwiQ)
  - 项目摘要：该项目的目标概述如下：1.在对流行的人工神经网络架构进行仔细转换的基础上，分析卷积钉刺神经网络（CSNN）进行图像分类的性能，这对Imagenet数据集的分类具有可喜的成果。2.在转换后的网络上执行增量依赖于尖峰时序的反向传播（STDB），以获得在几个时期内收敛并且需要较少时间步进行输入处理的SNN。3.在CIFAR-10和MNIST上针对VGG和Resnet体系结构进行实验。动机：近年来，Spiking神经网络（SNN）在通过事件驱动的神经形态硬件实现低功耗机器智能方面取得了巨大成功。二进制基于“全有或全无”峰值的通信加上稀疏的时间处理，使SNN成为传统ANN的低功耗替代品。尽管提高了能效，但训练SNN仍然是一个挑战，因为尖峰神经元（通常建模为泄漏的积分生火（LIF）或积分生火（IF ））与基于梯度下降的反向传播不兼容。在这项工作中，我们提出了一种混合训练技术，该技术结合了ANN-SNN转换和基于尖峰的反向传播，可减少总延迟并减少训练收敛所需的精力。我们使用ANN-SNN转换作为初始化步骤，然后进行基于尖峰的反向传播增量训练（由于前期初始化，收敛到最优精度，且历时很少）。从本质上讲，与仅通过转换或仅基于尖峰的反向传播从头训练的模型相比，采用转换的SNN并使用反向传播对其进行增量训练的混合方法可提高能源效率并提高准确性。数据需求和获取计划：我们计划逐步提高数据集的复杂性。我们计划在CIFAR-10和MNIST数据集上测试初始的ANN模型。这项工作的未来范围包括在ImageNet和CIFAR-100上进行测试。与仅通过转换或仅基于尖峰的反向传播从头开始训练的模型相比，我们采用混合SNN并使用反向传播对其进行增量训练的混合方法可提高能源效率并提高准确性。数据需求和获取计划：我们计划逐步提高数据集的复杂性。我们计划在CIFAR-10和MNIST数据集上测试初始的ANN模型。这项工作的未来范围包括在ImageNet和CIFAR-100上进行测试。与仅通过转换或仅基于尖峰的反向传播从头开始训练的模型相比，我们采用混合SNN并使用反向传播对其进行增量训练的混合方法可提高能源效率并提高准确性。数据需求和获取计划：我们计划逐步提高数据集的复杂性。我们计划在CIFAR-10和MNIST数据集上测试初始的ANN模型。这项工作的未来范围包括在ImageNet和CIFAR-100上进行测试。
  
- [20200601-普渡大学：RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency ](https://www.youtube.com/watch?v=IsAqBi3QniA)

  - 作者：Bing Han，Gopalakrishnan Srinivasan，Kaushik Roy描述：尖峰神经网络（SNN）最近引起了广泛的研究兴趣，因为第三代人工神经网络可以实现低功耗事件驱动的数据分析。通过将经过训练的模拟神经网络（ANN）（包括整流线性单位（ReLU））转换为由具有“适当”触发阈值的集成并发射神经元组成的SNN，可以获得用于图像识别任务的性能最好的SNN。与原始ANN相比，转换后的SNN通常会导致准确性损失，并且需要大量的推理时间步才能达到最佳准确性。我们发现，转换后的SNN的性能下降源于使用““硬重置”” 尖刺神经元，一旦其膜电位超过激发阈值，就会被驱动到固定的复位电位，从而导致SNN推理期间的信息丢失。我们提出使用“软重置”尖峰神经元模型的ANN-SNN转换，称为残留膜电位（RMP）尖峰神经元，该模型在触发时刻将““残留””膜电位保持在阈值以上。我们在具有挑战性的数据集（包括CIFAR-10（93.63％top-1），CIFAR-100（70.93％top--）上使用RMP神经元对VGG-16，ResNet-20和ResNet-34 SNN进行了近乎无损的ANN-SNN转换1）和ImageNet（73.09％的top-1准确性）。我们的结果还表明，RMP-SNN具有““硬重置”“，超过了转换后的SNN提供的最佳推理精度。
  
  
  ## 2、近期前沿论文
  
  - [arxiv](https://arxiv.org/search/?query=snn&searchtype=all&source=header)

